{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e950b18-766d-4357-b165-7fde3284dc5a",
   "metadata": {},
   "source": [
    "# GPT-Swarm\n",
    "\n",
    "[Language Agents as Optimizable Graphs](https://arxiv.org/pdf/2402.16823.pdf) by Zhuge, et. al out of Schmidhuber's lab frames agents as graphs (DAGs in their case), where each node is a unit of work (such as an LLM or tool), and the edges define channels where information can be transmitted between the LLMs. These graphs themselves can be composed into larger graphs that can behave as a swarm to accomplish a task.\n",
    "\n",
    "They then show how an agent can be _optimized_ to solve a given task given external rewards.\n",
    "\n",
    "They optimize the graph on two levels:\n",
    "\n",
    "1. Edge-level: They apply the classic [REINFORCE](https://link.springer.com/article/10.1007/BF00992696) algorithm to prune edges between nodes.\n",
    "2. Node-level: For Agent nodes, they prompt an LLM to update the prompt.\n",
    "\n",
    "\n",
    "\n",
    "This notebook will walk through how to implement the node-level optimization in LangGraph.\n",
    "\n",
    "The two main components here are :\n",
    "\n",
    "1. Node-level memory/history\n",
    "2. Optimizer\n",
    "\n",
    "\n",
    "<!-- Some things to bear in mind here: -->\n",
    "<!-- Graphs each have state. You typicaly don't want to mix outside class state with graph state. Best to put all in graph state (otherwise `.batch()` operations may do some funky things) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67010ea8-d5b6-4400-be15-f7581b997c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class LearnableState(TypedDict):\n",
    "    prompt: ChatPromptTemplate\n",
    "    \"\"\"The agent's prompt.\"\"\"\n",
    "    examples: list\n",
    "    \"\"\"Few-shot examples.\"\"\"\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    parameters: LearnableState\n",
    "    instructions: str\n",
    "\n",
    "\n",
    "async def get_new_prompt(negative_examples):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45036eaf-479e-4766-9993-90bf5dfb9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizableOperation(Node):\n",
    "    def __init__(\n",
    "        self,\n",
    "        domain: str,\n",
    "        combine_inputs_as_one: bool,\n",
    "        prompt: str,\n",
    "        model_name: Optional[str] = None,\n",
    "        operation_description: str = \"\",\n",
    "        id=None,\n",
    "        max_domenstrations: int = 4,\n",
    "    ):\n",
    "        self.domain = domain\n",
    "        self.model_name = model_name\n",
    "        self.llm = LLMRegistry.get(model_name)\n",
    "        super().__init__(operation_description, id, combine_inputs_as_one)\n",
    "        self.operation_description = operation_description\n",
    "        self.prompt = prompt\n",
    "        self.domenstrations = []\n",
    "        self.max_domenstrations = max_domenstrations\n",
    "\n",
    "    def get_complete_prompt(self, inputs):\n",
    "        pass\n",
    "\n",
    "    async def evaluate(self, candidate) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def get_new_prompt(self, negative_examples):\n",
    "        tasks = []\n",
    "        for negative_example in negative_examples:\n",
    "            meta_prompt = f\"\"\" Here is an example when {self.operation_description} gets wrong.\n",
    "Input:\n",
    "{negative_example['input']}\n",
    "------------------\n",
    "The output was:\n",
    "{negative_example['output']}\n",
    "------------------\n",
    "It received the following feedback:\n",
    "{negative_example['feedback']}\n",
    "\"\"\"\n",
    "            tasks.append(\n",
    "                self.llm.agen(\n",
    "                    [\n",
    "                        Message(role=\"user\", content=meta_prompt),\n",
    "                        Message(\n",
    "                            role=\"user\",\n",
    "                            content=f\"Identify a problem in {self.operation_description} from the given example and suggest how to prevent it without mentioning the specific example. Responde only one sentence.\",\n",
    "                        ),\n",
    "                    ],\n",
    "                    max_tokens=100,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        responds = await asyncio.gather(*tasks)\n",
    "        advice = \"\"\n",
    "        for i, respond in enumerate(responds):\n",
    "            advice += f\"{i + 1}. {respond}\\n\"\n",
    "\n",
    "        meta_prompt = f\"\"\"I'm trying to define {self.operation_description} by prompting.\n",
    "My current prompt is:\n",
    "\"{self.prompt}\"\n",
    "\n",
    "To generate an improved prompt, consider the following:\n",
    "{advice}\n",
    "Genergate an improved prompt within five sentences. Do not mention a specific task in the prompt!\n",
    "The prompt should be wrapped with <START> and <END>.\n",
    "\"\"\"\n",
    "        new_prompt = await self.llm.agen(\n",
    "            [Message(role=\"user\", content=meta_prompt)], max_tokens=200\n",
    "        )\n",
    "        new_prompt = new_prompt.split(\"<END>\")[0].split(\"<START>\")[-1].strip()\n",
    "        return new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b904fff-c2e0-4f6b-98ac-6d0fc0f9b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def optimize(\n",
    "    node: OptimizableOperation, learn_demonstration=False, learn_prompt=True\n",
    "):\n",
    "    examples = node.memory.query_by_id(node.id)[-4:]\n",
    "    positive_examples = [\n",
    "        example for example in examples if node.memory.query_by_id(example[\"task\"])[0]\n",
    "    ]\n",
    "    negative_examples = [\n",
    "        example\n",
    "        for example in examples\n",
    "        if not node.memory.query_by_id(example[\"task\"])[0]\n",
    "    ]\n",
    "\n",
    "    prompts = [node.prompt]\n",
    "    demonstrations = [node.domenstrations]\n",
    "    if learn_demonstration:\n",
    "        new_domenstrations = node.domenstrations + positive_examples\n",
    "        if len(new_domenstrations) > node.max_domenstrations:\n",
    "            new_domenstrations = random.sample(\n",
    "                new_domenstrations, node.max_domenstrations\n",
    "            )\n",
    "        demonstrations.append(new_domenstrations)\n",
    "\n",
    "    if learn_prompt and len(negative_examples) > 0:\n",
    "        new_prompt = await node.get_new_prompt(negative_examples)\n",
    "        prompts.append(new_prompt)\n",
    "\n",
    "    candidates = [\n",
    "        (prompt, domenstrations)\n",
    "        for prompt in prompts\n",
    "        for domenstrations in demonstrations\n",
    "    ]\n",
    "    if len(candidates) == 1:\n",
    "        return\n",
    "    tasks = [node.evaluate(candidate) for candidate in candidates]\n",
    "    scores = await asyncio.gather(*tasks)\n",
    "    node.prompt, node.domenstrations = candidates[scores.index(max(scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b72fa7-54ee-4a34-a965-4fb2ec8b82b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
